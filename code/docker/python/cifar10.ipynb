{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Cifar10Record(object):\n",
    "    width = 32\n",
    "    height = 32\n",
    "    depth = 3\n",
    "    \n",
    "    def set_label(self, label_byte):\n",
    "        self.label = np.frombuffer(label_byte, dtype=np.uint8)\n",
    "        \n",
    "    def set_image(self, image_bytes):\n",
    "        byte_buffer = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "        reshaped_array = np.reshape(byte_buffer,\n",
    "                                   [self.depth, self.height, self.width])\n",
    "        self.byte_array = np.transpose(reshaped_array, [1, 2, 0])\n",
    "        self.byte_array = self.byte_array.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Cifar10Reader(object):\n",
    "    def __init__(self, filename):\n",
    "        if not os.path.exists(filename):\n",
    "            print(filename + \" is not exist\")\n",
    "            return\n",
    "        \n",
    "        self.bytestream = open(filename, mode=\"rb\")\n",
    "        \n",
    "    def close(self):\n",
    "        if not self.bytestream:\n",
    "            self.bytestream.close()\n",
    "            \n",
    "    def read(self, index):\n",
    "        result = Cifar10Record()\n",
    "        \n",
    "        label_bytes = 1\n",
    "        image_bytes = result.height * result.width * result.depth\n",
    "        record_bytes = label_bytes + image_bytes\n",
    "        \n",
    "        self.bytestream.seek(record_bytes * index, 0)\n",
    "        \n",
    "        result.set_label(self.bytestream.read(label_bytes))\n",
    "        result.set_image(self.bytestream.read(image_bytes))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CIFAR 10 Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 6\n",
      "label: 9\n",
      "label: 9\n",
      "label: 4\n",
      "label: 1\n",
      "label: 1\n",
      "label: 2\n",
      "label: 7\n",
      "label: 8\n",
      "label: 3\n",
      "label: 4\n",
      "label: 7\n",
      "label: 7\n",
      "label: 2\n",
      "label: 9\n",
      "label: 9\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "#tf.app.flags.DEFINE_string(\"file\", \"./data/cifar-10-batches-bin/data_batch_1.bin\", \"\")\n",
    "#tf.app.flags.DEFINE_integer(\"offset\", 0, \"\")\n",
    "#tf.app.flags.DEFINE_integer(\"length\", 16, \"\")\n",
    "offset = 0\n",
    "length = 16\n",
    "\n",
    "basename = os.path.basename(\"./data/cifar-10-batches-bin/data_batch_1.bin\")\n",
    "path = os.path.dirname(\"./data/cifar-10-batches-bin/data_batch_1.bin\")\n",
    "\n",
    "reader = Cifar10Reader(\"./data/cifar-10-batches-bin/data_batch_1.bin\")\n",
    "\n",
    "stop = offset + length\n",
    "\n",
    "for index in range(offset, stop):\n",
    "    image = reader.read(index)\n",
    "    \n",
    "    print(\"label: %d\" % image.label)\n",
    "    imageshow = Image.fromarray(image.byte_array.astype(np.uint8))\n",
    "    \n",
    "    file_name = \"%s-%02d-%d.png\" % (basename, index, image.label)\n",
    "    file = os.path.join(path, file_name)\n",
    "    with open(file, mode=\"wb\") as out:\n",
    "        imageshow.save(out, format=\"png\")\n",
    "\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import time\n",
    "import model\n",
    "\n",
    "EPOCH = 30\n",
    "data_dir = \"./data/cifar-10-batches-bin/\"\n",
    "checkpoint_dir = \"./checkpoint_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-213f01b35c0a>:9 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "[6]: [array([[-0.00699217,  0.01172052, -0.01157834,  0.00869165,  0.01330956,\n",
      "         0.00492495,  0.01903891, -0.00826489, -0.0068655 , -0.02597469]], dtype=float32)]\n",
      "[9]: [array([[-0.00912614,  0.01163021, -0.01306129,  0.00974152,  0.01392974,\n",
      "         0.00436251,  0.01797695, -0.0100047 , -0.0074282 , -0.02633977]], dtype=float32)]\n",
      "[7]: [array([[-0.01073989,  0.01089748, -0.01282292,  0.00886368,  0.01384023,\n",
      "         0.00374595,  0.01829774, -0.00858094, -0.00683134, -0.02643224]], dtype=float32)]\n",
      "[3]: [array([[-0.00997386,  0.01237816, -0.01174688,  0.00993662,  0.01397108,\n",
      "         0.00557554,  0.01733606, -0.00875834, -0.00772903, -0.02669028]], dtype=float32)]\n",
      "[5]: [array([[-0.00829392,  0.01130554, -0.01222941,  0.00880565,  0.01381272,\n",
      "         0.00574741,  0.01981927, -0.01079153, -0.00700656, -0.02818566]], dtype=float32)]\n",
      "[6]: [array([[-0.00925035,  0.01144531, -0.01079876,  0.00789323,  0.014344  ,\n",
      "         0.00392952,  0.0171416 , -0.00721516, -0.00736213, -0.02498438]], dtype=float32)]\n",
      "[6]: [array([[-0.00864532,  0.01185326, -0.01127892,  0.00868765,  0.01404675,\n",
      "         0.00437748,  0.01962881, -0.00866581, -0.00599326, -0.02716069]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-213f01b35c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                 logits_value = sess.run([logits],\n\u001b[1;32m     24\u001b[0m                                        feed_dict={\n\u001b[0;32m---> 25\u001b[0;31m                                            \u001b[0mtrain_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                                        })\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    916\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 918\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    919\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2317\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2318\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "     train_placeholder = tf.placeholder(tf.float32,\n",
    "                                      shape=[32, 32, 3], name=\"input_image\")\n",
    "     image_node = tf.expand_dims(train_placeholder, 0)\n",
    "\n",
    "     logits = model.inference(image_node)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    total_duration = 0\n",
    "    \n",
    "    for epoch in range(1, EPOCH):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for file_index in range(5):\n",
    "            print(\"Epoch %d: %s\" % (epoch, filenames[file_index]))\n",
    "            reader = Cifar10Reader(filenames[file_index])\n",
    "            \n",
    "            for index in range(10000):\n",
    "                image = reader.read(index)\n",
    "                \n",
    "                logits_value = sess.run([logits],\n",
    "                                       feed_dict={\n",
    "                                           train_placeholder: image.byte_array,\n",
    "                                       })\n",
    "                if index % 1000 == 0:\n",
    "                    print(\"[%d]: %r\" %(image.label, logits_value))\n",
    "                    \n",
    "                reader.close()\n",
    "                \n",
    "            duration = time.time() - start_time\n",
    "            total_duration += duration\n",
    "            \n",
    "            print('epoch %d duration = %d sec' % (epoch, duration))\n",
    "            \n",
    "            tf.train.SummaryWriter(checkpoint_dir, sess.graph)\n",
    "            \n",
    "        print('Total duration = %d sec' % total_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}