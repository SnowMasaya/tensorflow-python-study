{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model\n",
    "from reader import Cifar10Reader\n",
    "\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 30\n",
    "LEARNING_RATE = 0.001\n",
    "data_dir = \"./data/cifar-10-batches-bin/\"\n",
    "check_point_dir = \"./check_point_dir/\"\n",
    "test_data = \"./data/cifar-10-batches-bin/test_batch.bin\"\n",
    "graph_dir = \"./graph_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loss(logits, label):\n",
    "    labels = tf.cast(label, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name=\"cross_entropy_per_example\")\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\"cross_entropy\")\n",
    "    return cross_entropy_mean\n",
    "\n",
    "def _train(total_loss, global_step):\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "    train_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "  os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _eval(sess, top_k_op, input_image, label_placeholder):\n",
    "    if not test_data:\n",
    "        return np.nan\n",
    "    \n",
    "    image_reader = Cifar10Reader(test_data)\n",
    "    true_count = 0\n",
    "    for index in range(10000):\n",
    "        image = image_reader.read(index)\n",
    "        \n",
    "        predictions = sess.run([top_k_op],\n",
    "                              feed_dict={\n",
    "                                  input_image: image.byte_array,\n",
    "                                  label_placeholder: image.label\n",
    "                              })\n",
    "        true_count += np.sum(predictions)\n",
    "    image_reader.close()\n",
    "    \n",
    "    return (true_count / 10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _restore(saver, sess):\n",
    "    checkpoint = tf.train.get_checkpoint_state(check_point_dir)\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        save.restore(sess, checkpoint.model_checkpoint_path)\n",
    "\n",
    "def _export_graph(sess, epoch):\n",
    "    constant_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, sess.graph_def, [\"output/logits\"])\n",
    "    \n",
    "    file_path = os.path.join(graph_dir, 'graph_%02d_epoch.pb' % epoch)\n",
    "    with gfile.FastGFile(file_path, \"wb\") as f:\n",
    "        f.write(constant_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-87751ea57162>:15 in <module>.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "with tf.device('/gpu:0'):\n",
    "    train_placeholder = tf.placeholder(tf.float32, shape=[32, 32, 3],\n",
    "                                   name=\"input_image\")\n",
    "    label_placeholder = tf.placeholder(tf.int32, shape=[1], name=\"label\")\n",
    "\n",
    "    image_node = tf.expand_dims(train_placeholder, 0)\n",
    "\n",
    "    logits = model.inference(image_node)\n",
    "    total_loss = _loss(logits, label_placeholder)\n",
    "    train_op = _train(total_loss, global_step)\n",
    "\n",
    "top_k_op = tf.nn.in_top_k(logits, label_placeholder, 1)\n",
    "\n",
    "saver = tf.train.Saver(tf.all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-f7a8cc79ee36>:2 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "INFO:tensorflow:Froze 22 variables.\n",
      "Converted 10 variables to const ops.\n",
      "EPOCH 1: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 1: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 1: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 1: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 1: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 2: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 2: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 2: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 2: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 2: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 3: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 3: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 3: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 3: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 3: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 4: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 4: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 4: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 4: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 4: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 5: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 5: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 5: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 5: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 5: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 6: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 6: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 6: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 6: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 6: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 7: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 7: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 7: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 7: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 7: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 8: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 8: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 8: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 8: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 8: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 9: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 9: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 9: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 9: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 9: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 10: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 10: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 10: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 10: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 10: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 11: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 11: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 11: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 11: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 11: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 12: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 12: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 12: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 12: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 12: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 13: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 13: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 13: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 13: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 13: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 14: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 14: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 14: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 14: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 14: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 15: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 15: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 15: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 15: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 15: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 16: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 16: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 16: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 16: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 16: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 17: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 17: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 17: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 17: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 17: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 18: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 18: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 18: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 18: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 18: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 19: ./data/cifar-10-batches-bin/data_batch_1.bin\n",
      "EPOCH 19: ./data/cifar-10-batches-bin/data_batch_2.bin\n",
      "EPOCH 19: ./data/cifar-10-batches-bin/data_batch_3.bin\n",
      "EPOCH 19: ./data/cifar-10-batches-bin/data_batch_4.bin\n",
      "EPOCH 19: ./data/cifar-10-batches-bin/data_batch_5.bin\n",
      "EPOCH 20: ./data/cifar-10-batches-bin/data_batch_1.bin\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    total_duration = 0\n",
    "    \n",
    "    _export_graph(sess, 0)\n",
    "    \n",
    "    for epoch in range(1, EPOCH + 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for file_index in range(5):\n",
    "            print(\"EPOCH %d: %s\" % (epoch, filenames[file_index]))\n",
    "            reader = Cifar10Reader(filenames[file_index])\n",
    "            \n",
    "            for index in range(10000):\n",
    "                image = reader.read(index)\n",
    "                \n",
    "                _, loss_value = sess.run([train_op, total_loss],\n",
    "                                        feed_dict={\n",
    "                                            train_placeholder: image.byte_array,\n",
    "                                            label_placeholder: image.label\n",
    "                                        })\n",
    "                assert not np.isnan(loss_value)\n",
    "                \n",
    "            reader.close()\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    total_duration += duration\n",
    "    \n",
    "    prediction = _eval(sess, top_k_op,\n",
    "                       train_placeholder, label_placeholder)\n",
    "    print(\"epoch %d duration = %d sec, prediction = %.3f\"\n",
    "          % (epoch, duration, prediction))\n",
    "    \n",
    "    tf.train.SummaryWriter(check_point_dir, sess.graph)\n",
    "    sarver.save(sess, check_point_dir, global_step=epoch)\n",
    "    _export_graph(sess, epoch)\n",
    "    \n",
    "print(\"Total duration = %d sec\" % total_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}